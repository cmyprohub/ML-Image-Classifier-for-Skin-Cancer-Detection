---
title: 'Computer Vision & ML -  Skin Cancer Diagnostics'
date: ' Dec 2019'
output:
  word_document:
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes: \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
 # knitr::opts_chunk$set(include = TRUE)  

  #knitr::opts_chunk$set(echo = TRUE)
  knitr::opts_chunk$set(message = FALSE)
  knitr::opts_chunk$set(warning = FALSE)
  knitr::opts_chunk$set(fig.height = 6, fig.width = 8, out.width = '50%', fig.align = "center")
  options(width = 90)
```

```{css, echo=FALSE}
.solution {
background-color: #e6ffe6;
}
```
## 1. Project Description and Summary (1 page)

* **Goal**
  Goal of the project is to:
  -  download and process 300 annotated images of skin moles in JPEG format, 150 of which are benign and the remaining 150 malignant.
  - construct 3  different classification models for identifying malignant moles based on the pixels (RGB colors) of these images and demonstrate the model accuracies.
  - define new data processing/feature engineering approaches that could improve the interpretability of the machine learning classification models.
  - apply 2 different classification models on the feature engineered skin image data and demonstrate that the model accuracies have increased.

* **Approach**
 - Load the images using EBImage library. EBImage provides general purpose functionality for image processing,analysis and facilitates statistical modeling, machine learning and visualization with image data.

 - For Part 1, considering the fact that all the images are not of the same size,resize all the images to 256 X 256 X 3 (3 channels - RGB) and vectorize the image arrays for further processing.

 - For Part 1, define 3 pixel based machine learning models based on the following algorithms - Gradient Boosting Machines(GBM), K-Nearest Neighbor(KNN), Naive-Bayes (NB) and calculate the accuracies on Train and Test data (70 - 30 split)

 - For Part 2, perform the following additional image processing and feature engineering steps using EBImage package before vectorizing the image data and using it for classification.
  1. image brightness,contrast adjustment
  2. adaptive image thresholding
  4. image resizing to 256 X 256 X 3
  5. image cropping (to size 200 X 200 X 3) to remove unwanted image borders
  6. Principal Component Analysis (PCA) to select the most prominant features for further machine
     learning
  7. Compute additional shape features using EBImage Package
  8. Use computed shape features to derive border additional features - irregularity index and
    geometrical asymmetry measure

 - For Part 2, fit 2  machine learning models - K-Nearest Neighbor(KNN), Gradient Boosting Machines(GBM) on the feature engineered data and calculate the accuracies on Train and Test data (70 - 30 split)

* **Results**
Results obtained show that additional feature engineering, especially those engineered based on  clinical papers and domain knowledge of the skin cancer definitely contribute towards increasing the classification accuracy when it comes to identifying malignant skin lesions versus benign ones, thus aiding more in the detection of skin cancer.

The below details based on which this conclusion has been made are also captured in this report.
 - Classification Results and Metrices for Part 1 - All 3 Models
 - Classification Results and Metrices for Part 2 - Both the Models
 - Comparison of Model classification results for Part 1 and  Part 2

## 2. Data Processing (for Part 1)
Data Processing (for Part 1) steps:
Use R's EBImage library to load and resize all the images to 256 X 256 X 3 (3 channels - RGB) and vectorize the image arrays for further processing as shown below

```{r echo=TRUE, eval=TRUE}
################################################################
#Benign Image Processing Using EBImage library(for Part 1)
################################################################
library(EBImage) #image processing library
ben.files <- list.files("Data/benign") #list of all benign image files
mal.files <- list.files("Data/malignant") #list of all malignant image files
ben.df <- data.frame()
for (i in 1:150) {
  ben.path <- paste("Data/benign/", ben.files[i], sep="")
  b.img <- readImage(ben.path)
  rz.b.img <-  resize(b.img, w=256, h=256)# #resize each image
  rz.bimg.vec <- as.vector(rz.b.img) #vectorize
  labelled.bimg.vec <- c(0, rz.bimg.vec) #add a new column with '0' as label
  #append to the dataframe
  ben.df <- rbind(ben.df, labelled.bimg.vec) }
#name all the columns in the dataframe
names(ben.df) <- c("label", 1:(ncol(ben.df)-1))

```

```{r echo=FALSE, eval=TRUE}
################################################################
#Malignant Image Processing Using EBImage library(for Part 1)
################################################################
mal.df <- data.frame()
for (i in 1:150) {
  mal.path <- paste("Data/malignant/", mal.files[i], sep="")
  m.img <- readImage(mal.path)
  rz.m.img <-  resize(m.img, w=256, h=256) # resize each image
  rz.mimg.vec <- as.vector(rz.m.img)   #vectorize
  labelled.mimg.vec <- c(1, rz.mimg.vec)  #add a new column with '1' as label
  mal.df <- rbind(mal.df, labelled.mimg.vec) }
names(mal.df) <- c("label", 1:(ncol(mal.df)-1))

#combine both the malignant and benign dataframes into one
all.img.df <- rbind(ben.df, mal.df)
```

```{r echo=FALSE, eval=TRUE, include=TRUE, out.width = '100%'}
###################################################################
# Display pixel intensities plotted in a histogram (for Part 1)
###################################################################
library(EBImage) #image processing library
ben.files <- list.files("Data/benign") #list of all benign image files
mal.files <- list.files("Data/malignant") #list of all malignant image files
ben.path <- paste("Data/benign/", ben.files[25], sep="")
b.img <- readImage(ben.path)
mal.path <- paste("Data/malignant/", mal.files[25], sep="")
m.img <- readImage(mal.path)
rz.b.img <-  resize(b.img, w=256, h=256)
rz.m.img <-  resize(m.img, w=256, h=256)
#distribution  of pixel intensities plotted in a histogram
par(mfrow = c(1,4))
hist(m.img, main = "Pixel Intensity of\n a Malignant Image")
hist( rz.m.img, main = "Pixel Intensity \nMalignant Image(Resized)")
hist(b.img, main = "Pixel Intensity of \a nBenign Image")
hist(rz.b.img, main = "Pixel Intensity of\n a Benign Image(Resized)")
```

```{r  echo=FALSE, eval=TRUE, include = FALSE}
################################################################
# Test-Train split (for Part 1)
################################################################
#save to a  csv for ease of use
#write.csv(all.img.df, file="processed.csv", row.names=FALSE)

#load processed image data to df from csv
#all.img.df <- read.csv("processed-allpixels.csv", sep=",")

pca1 <- prcomp(all.img.df[, 2:length(all.img.df)])

inter = cbind(all.img.df[,1],pca1$x)
#shuffle the dataset,Test - Train split. 30% becomes test data
set.seed(0)
img.df.schuff <- inter[sample(1:nrow(inter)),]

TestRow1 = seq(1:90)
TestRow1= c(TestRow1)
Test1 = img.df.schuff[TestRow1,]
Train1 = img.df.schuff[-TestRow1,]
xTrain1 = as.matrix(Train1[, -1])
yTrain1 = as.matrix(Train1[, 1])
xTest1 = as.matrix(Test1[, -1])
yTest1 = as.matrix(Test1[, 1])

```


## 3. Classification Models Based on Pixels (for Part 1)

### 3.1 Model 1
* **Model Details & Tuning**
First classifier model used for identifying malignant skin cancer is Naïve Bayes classifier which is a simple probabilistic classifier  based on Bayes theorem which is based on concept of variable  independence. Trainer from caret package is used; parameter ‘nb’ indicates to use Naive Bayes. The trainControl part tells the trainer to use cross-validataion (‘cv’) with 10 folds. xTrain 1 is the train data having pixel values and yTrain1 has labels indicating '0' for benign and '1' for malignant images. Model will be fitted on xTest1 data and predictions will be done classsifying each image as benign (0) or malignant(1).

Details of the model and prediction statistics are given below:

```{r echo=TRUE, eval=TRUE, include = TRUE}
################################################################
#Part 1 - Model 1: Naive Bayes Classifier Details & Tuning
################################################################
  library(e1071)
  library(caret)
  set.seed(45)
  #fitting the NB classifier on train data
  q1nb.fit = train(xTrain1, as.factor(yTrain1),'nb',trControl=trainControl(method='cv',number=5))
  q1nb.fit

```


* **Model Validation**
```{r echo=FALSE, eval=TRUE, include = TRUE}

################################################################
  #Part 1 - Model 1: Naive_Bayes Classifier Validation
################################################################
  #Model Evaluation
  #Predict on test data & train data
  q1nb.yPredTest <- predict(q1nb.fit,newdata = xTest1 )
  q1nb.yPredTrain <- predict(q1nb.fit,newdata = xTrain1 )

  #Test - Confusion matrix to see accuracy value and other parameter values
  q1nb.tscm = confusionMatrix(q1nb.yPredTest,as.factor( yTest1) )
  cat("Part 1 NB - TEST DATA PREDICTION STATISTICS")
  q1nb.tscm
  #Train - Confusion matrix to see accuracy value and other parameter values
  q1nb.trcm  = confusionMatrix(q1nb.yPredTrain,as.factor( yTrain1) )
  #cat("Part 1 NB - TRAIN DATA")
  #q1nb.trcm
  #Accuracy on test & train
  q1nb.test.acc  = 1- (sum(yTest1 != q1nb.yPredTest) / length(q1nb.yPredTest))
  q1nb.train.acc  = 1- (sum(yTrain1 != q1nb.yPredTrain) / length(q1nb.yPredTrain))

```



### 3.2 Model 2
* **Model Details and Tuning**
Second classifier  used for identifying malignant skin cancer is K-Nearest Neighbor(KNN) classifier
 which is a non-parametric method. Here knn from 'class' package is used and the output is a class membership - benign (0) or malignant(1). An object is classified based on vote from its neighbors, with the object being assigned to the class most common among its k nearest neighbors.

A common rule of thumb while tuning parameter 'k' is to use a rounded value of the square root of total number of elements in the dataset on which the classifier will be fit. In this case that would be square root of 210 = 14.4 (hence k = 14 is chosen). This choice of k  proved to be the best in terms of accuracy based on trial on multiple k values.

Details of the model and prediction statistics are given below:

```{r echo=TRUE, eval=TRUE, include = TRUE}
########################################################################
#Part 1 - Model 2: K Nearest Neighbor (KNN) - Model Details & Tuning
#########################################################################
library(class)
#fit KNN model on train data and predict ; number of classes = 2
#Model fitting
set.seed(10)
q1knn.yPredTrain <- knn(train = xTrain1, test = xTrain1, cl=yTrain1 , k=14)
q1knn.yPredTest <- knn(train = xTrain1, test = xTest1, cl=yTrain1 , k=14)
```

* **Model Validation**
```{r echo=FALSE, eval=TRUE, include = TRUE}
################################################################
#Part 1 - Model 2: K Nearest Neighbor (KNN)
################################################################
library(caret)
#prediction and accuracy - test and train data
q1knn.train.acc  = 1- (sum(yTrain1 != q1knn.yPredTrain) / length(q1knn.yPredTrain))
q1knn.test.acc  = 1- (sum(yTest1 != q1knn.yPredTest) / length(q1knn.yPredTest))

#Test - Confusion matrix to see accuracy value and other parameter values
q1knn.tscm = confusionMatrix(q1knn.yPredTest,as.factor( yTest1) )
cat("Part 1 KNN - TEST DATA PREDICTION STATISTICS")
q1knn.tscm
#Train - Confusion matrix to see accuracy value and other parameter values
q1knn.trcm = confusionMatrix(q1knn.yPredTrain,as.factor( yTrain1) )
#q1knn.trcm
```


### 3.3 Model 3
* **Model Details and Tuning**
Third classifier  used for identifying malignant skin cancer is Gradient Boosting classifier (GBM)
 which is an ensemble  method. The output is a class membership - benign (0) or malignant(1). Gradient boosting classifiers produces a prediction model in the form of an ensemble of weak prediction models called stumps.

The  gbm() function from 'gbm' package has been used in the code below. This uses a learning rate (shrinkage) of 0.001,default number of trees of 100 with default depth of each tree (interaction.depth) 1, which means we are ensembling a bunch of stumps. A cross validation using  cv.folds is used to to perform a 10 fold cross validation.

Details of the model and prediction statistics are given below:

```{r echo=TRUE, eval=FALSE, include = TRUE}
##########################################################################################
#Part 1 - Model 3: GBM (Gradient Boosting Machines) Classifier - Model Details& Tuning
##########################################################################################
library(caret)
library(gbm)
set.seed(0)
#fitting the NB classifier on train data
q1gb.fit = train(xTrain1,as.factor(yTrain1),'gbm',trControl=trainControl(method='cv',number=10))
q1gb.fit
```
```{r echo=FALSE, eval=TRUE, include = FALSE}
##########################################################################################
#Part 1 - Model 3: GBM (Gradient Boosting Machines) Classifier - Model Details& Tuning
##########################################################################################
library(caret)
library(gbm)
set.seed(0)
#fitting the NB classifier on train data
q1gb.fit = train(xTrain1,as.factor(yTrain1),'gbm',trControl=trainControl(method='cv',number=10))
q1gb.fit
```
* **Model Validation**
```{r echo=FALSE, eval=TRUE, include = TRUE}
######################################################################################
#Part 1 - Model 3: GBM (Gradient Boosting Machines) Classifier - Model Evaluation
######################################################################################
#Predict on test data & train data
q1gb.yPredTest <- predict(q1gb.fit,newdata = xTest1 )
q1gb.yPredTrain <- predict(q1gb.fit,newdata = xTrain1 )

#Test and train - Confusion matrix to see accuracy value and other parameter values
q1gb.tscm = confusionMatrix(q1gb.yPredTest,as.factor( yTest1) )
cat("Part 1 GBM - TEST DATA PREDICTION STATISTICS")
q1gb.tscm
q1gb.trcm = confusionMatrix(q1gb.yPredTrain,as.factor( yTrain1) )
#q1gb.trcm

#Accuracy on test & train
q1gb.test.acc  = 1- (sum(yTest1 != q1gb.yPredTest) / length(q1gb.yPredTest))
q1gb.train.acc  = 1- (sum(yTrain1 != q1gb.yPredTrain) / length(q1gb.yPredTrain))

```

## 4. Literature Review  for Part 2

Below is a summary of literature review that has motivated the feature engineering for Part 2:

* **1.American Cancer Society**  (http://www.cancer.org/cancer/melanoma-skin-cancer.html)

 American Cancer Society has defined the ABCDE rule as a guide to check if a particular skin lesion is benign (non-cancerous) or malignant (melanoma or skin cancer).

Melanoma spots often seem to have one or more  of the following features:
● A is for Asymmetry: One half of a mole or birthmark does not match the other.
● B is for Border:The edges are irregular, ragged, notched, or blurred.
● C is for Color:The color is not the same all over and may include different shades
of brown or black, or sometimes with patches of pink, red, white, or blue.
● D is for Diameter:The spot is larger than 6 millimeters across (about ¼ inch – the
size of a pencil eraser), although melanomas can sometimes be smaller than this.
● E is for Evolving: The mole is changing in size, shape, or color.

* **2.NIH Clinical Paper** - Title: 'A systematic heuristic approach for feature selection for melanoma discrimination using clinical images' (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3193077/)

Of the ABCDE clinical features mentioned above, the above clinical paper discusses about how the shape features of the lesions identified from clinical images, particularly 'A' and 'B' features - asymmetry and border irregularity - can be used to derive features that could be used to classify  skin cancer images.

The paper proposes the use of below computed shape features based on quantifying border irregularity and lesion asymmetry.
(a) **Border irregularity index**
In order to quantify border irregularity, an irregularity index is computed for each lesion. The irregularity index is given by the formula - I = (ab / 2pi((a^2)+(b^2))  (P^2/A)
where  a and b are the lengths of the major and minor axes of the best-fit ellipse, respectively, P is the perimeter of the lesion border, and A is the area of the lesion.
This could be used a a new feature by the classification model for skin cancer identification from images.


* **3.Journal of Artificial Intelligence** Article 'Asymmetry Analysis of Malignant Melanoma Using Image Processing: A Survey' from  (https://scialert.net/fulltextmobile/?doi=jai.2014.45.53#e5)

The above article was referred which proposed the use of geometrical symmetry as a feature to identify skin cancer lesions.

(b) **Geometrical asymmetry**
  Geometric asymmetry can be found by dividing the lesion into 2 parts by straight line that passes through the center of mass, after that a comparison is made between the 2 parts by calculating the distance present between the size functions. This size functions distance also determines the qualitative asymmetry.

  Based on this a geometrical asymmetry feature can be calculated as 'difference between minimum radius of the lesion and maximum raduis of the lesion' and this can also be used as a new feature by the calssification model for skin cancer identification from images.

## 5. Feature Engineering for Part 2

Below are the main feature engineering steps that were done as part of Part 2

* Calculation of **border irregularity index** feature. This value that quantifies the border irregularity of the lesion was calculated using the formula - I = (ab / 2pi(a^2+b^2))  (P^2/A) - based on the medical literature mentioned above. The axes values , area and perimeter values were calculated using the EBImage package 'computeFeatures.shape()' which returns  the following shape related features of the lesion - A = s.area, P = s.perimeter, b = s.radius.min, a = s.radius.max

* Calculation of **geometrical asymmetry measure ** feature. This value that quantifies the asymmetry of the lesion was calculated using the formula - GA = (max raduis - min radius) - based on the medical literature mentioned above. The radius values were calculated using the EBImage package 'computeFeatures.shape()' which returns  the following shape related features of the lesion - min radius =  s.radius.min, max radius = s.radius.max

* The above two derived features, along with the **area** and **perimeter** features returned by the EBImage function computeFeatures.shape() were used for the calssification process

* In addition to the above, the actual image itself was subjected to the following **image enhancement** steps using EBImage package making it easier to identify key features of the lesion when extracted , compared to the surrounding skin area.

      1. **Brightness and Contrast adjustment** - Adjusting brightness and contrast makes sure that the lesion is clearly visible in the image compared to the surroundings. Brightness and contrat can be increased by adding or multiplying a positive value to the image respectively - (img + 0.6) or
 (img * 2)

      2. **Adaptive Image Thresholding** - EBImage's threshold() function separate "object" or foreground pixels from background pixels to aid in image processing.It also nullifies the affect of uneven illumination or by stray signal from nearby bright objects.

      3. **Resizing** - EBImage function resize() performs resizing by scaling the image x to the desired dimensions.This was important as not all of our images were of the exact size

      4. **Crop Images** - Images were also uniformly cropped to reduce the noise line presence of borders in some cases.

      5. **PCA** - Principal Component Analysis or PCA , a dimensionality reduction procedure was also done on these images post enhancement and vectorization to make sure that only the most prominant of almost the 120k pixels were selected to be used for the classification model.
```{r echo=TRUE,  eval = TRUE}
###################################################################################
#Raw Image Processing and Feature Engineering Using EBImage library(for Part 2)
###################################################################################
# - Lesion shape feature extraction & Feature Engineering - benign
library(EBImage) #image processing library
#Load benign images
ben.df <- data.frame()
for (i in 1:150) {
  ben.path <- paste("Data/benign/", ben.files[i], sep="")
  b.img <- readImage(ben.path)
  gray.b.img<-channel(b.img,"gray")
  b.img.ft = computeFeatures.shape(x=bwlabel(gray.b.img)) # access shape features
  #Border irregularity index - b.irr.f
  b.area = b.img.ft [1]
  A = b.area #area
  b.perimeter = b.img.ft [2]
  P = b.perimeter #perimeter
  b.major.ell.axlen = b.img.ft [6] * 2
  a = b.major.ell.axlen  #major axis length
  b.minor.ell.axlen = b.img.ft [5] * 2
  b = b.minor.ell.axlen #minor axis length
  b.num = (( a * b ) / (2*3.14*(a^2 + b^ 2)))
  b.den = (P^2 / A)
  b.irr.f = b.num/b.den
  #geometrical asymmetry measure
  g.asy.m = (b.img.ft [6] - b.img.ft [5])
  #combining all shape features
  b.img.f = cbind(A,P,g.asy.m ,b.irr.f)
  #vectorize
  rz.bimg.vec <- as.vector(b.img.f)
  #add a new column with '0' as label
  labelled.bimg.vec <- c(0, rz.bimg.vec)
  #append to the dataframe
  ben.df <- rbind(ben.df, labelled.bimg.vec)
}
#name all the columns in the dataframe
names(ben.df) <- c("label", 1:(ncol(ben.df)-1))

```


```{r echo=FALSE, eval = TRUE}
############################################################################
## Part 2 - Feature Engineering Image Enhancement & complete pixel load
############################################################################
# - Lesion shape feature extraction & Feature Engineering - malignant
mal.df <- data.frame()
for (i in 1:150) {
  mal.path <- paste("Data/malignant/", mal.files[i], sep="")
  m.img <- readImage(mal.path)
  gray.m.img<-channel(m.img,"gray")
  m.img.ft = computeFeatures.shape(x=bwlabel(gray.m.img))
  m.area = m.img.ft [1]
  A = m.area #area
  m.perimeter = m.img.ft [2]
  P = m.perimeter #perimeter
  m.major.ell.axlen = m.img.ft [6] * 2
  a = m.major.ell.axlen #major axis length
  m.minor.ell.axlen = m.img.ft [5] * 2
  b = m.minor.ell.axlen #minor axis length
  m.num = (( a * b ) / (2*3.14*(a^2 + b^ 2)))
  m.den = (P^2 / A)
  m.irr.f = m.num/m.den #border irregularity index
  g.asy.m = m.img.ft [6] - m.img.ft [5] #geometrical asymmetry measure (max radius - min radius)

  m.img.f = cbind(A,P, g.asy.m,m.irr.f)

  #vectorize
  rz.mimg.vec <- as.vector(m.img.f)

  #add a new column with '1' as label
  labelled.mimg.vec <- c(1, rz.mimg.vec)
  #append to the dataframe
  mal.df <- rbind(mal.df, labelled.mimg.vec)
}
#name all the columns in the dataframe (label = first col)
names(mal.df) <- c("label", 1:(ncol(mal.df)-1))

#combine both the malignant and benign dataframes into one
all.img.df <- rbind(ben.df, mal.df)

############################################################################
## Part 2 - Feature Engineering Image Enhancement & complete pixel load
############################################################################
ben.df <- data.frame()
for (i in 1:150) {
  ben.path <- paste("Data/benign/", ben.files[i], sep="")
  b.img <- readImage(ben.path)
  b.img <-  b.img + 0.6
  b.img <-  (b.img *2)
  b.img = thresh(b.img, 10, 10, 0.05)

  #resize each image
  rz.b.img <-  resize(b.img, w=256, h=256)
  rz.b.img.crop  = rz.b.img [31:230,31:230,]

  #vectorize
  rz.bimg.vec <- as.vector(rz.b.img.crop)
  #add a new column with '0' as label
  labelled.bimg.vec <- c(0, rz.bimg.vec)
  #append to the dataframe
  ben.df <- rbind(ben.df, labelled.bimg.vec)
}
#name all the columns in the dataframe
names(ben.df) <- c("label", 1:(ncol(ben.df)-1))

mal.df <- data.frame()
for (i in 1:150) {
  mal.path <- paste("Data/malignant/", mal.files[i], sep="")
  m.img <- readImage(mal.path)
  m.img <-  m.img + 0.6
  m.img <-  (m.img * 2)
  m.img = thresh(m.img, 10, 10, 0.05)

  #resize each image
  rz.m.img <-  resize(m.img, w=256, h=256)
  rz.m.img.crop  = rz.m.img [31:230,31:230,]

  #vectorize
  rz.mimg.vec <- as.vector(rz.m.img.crop)

  #add a new column with '1' as label
  labelled.mimg.vec <- c(1, rz.mimg.vec)
  #append to the dataframe
  mal.df <- rbind(mal.df, labelled.mimg.vec)
}
#name all the columns in the dataframe (label = first col)
names(mal.df) <- c("label", 1:(ncol(mal.df)-1))

#combine both the malignant and benign dataframes into one
all.img.df.p <- rbind(ben.df, mal.df)

######################################################################
#PCA Diamensionality Reduction - Feature Engineering (for Part 2)
######################################################################

pca <- prcomp(all.img.df.p[, 2:length(all.img.df.p)])

#plot(pca$x, pch=20, col=ifelse(all.img.df.p[,1]>0,"red", "blue")) # To plot dots, drop type="n"
#text(pca$x, rownames(pca$x), cex=0.8)

#combine pixel pca data and non pixel shape features
img.df.allfeat <- cbind(all.img.df, pca$x[,1:40])

#save to a  csv for ease of use
#write.csv(img.df.allfeat, file="processed-allfeat.csv", row.names=FALSE)

```

```{r echo=FALSE,  eval = TRUE}
#####################################
# Part 2 - Data Test train split
####################################

#load processed image data to df from csv
#img.df.allfeat <- read.csv("processed-allfeat.csv", sep=",")

#schuffle the feature rows
set.seed(0)
img.df.schuff.p <- img.df.allfeat[sample(1:nrow(img.df.allfeat)),]

#pca$x[,1] #first col of PC1
#Test - Train split. 30% becomes test data
#ok to choose first 30 as the data is already shuffled
TestRow2 = seq(1:90)
TestRow2 = c(TestRow2)
xTrain2 = as.matrix(img.df.schuff.p[-TestRow2,2:30] )
yTrain2 = as.matrix(img.df.schuff.p[-TestRow2, 1])
xTest2 = as.matrix(img.df.schuff.p[TestRow2,2:30 ])
yTest2 = as.matrix(img.df.schuff.p[TestRow2, 1])

```

## 6. 2 CLASSIFICATION MODELS for Part 2
### 6.1 Model 1
* **Model Details and Tuning**
Model 2 for Part 2 is a Gradient Boosting classifier (GBM) classifier.

Parameters and settings chosen are same as that of GBM in Part 1, for comparison purpose.

Details of the model and prediction statistics are given below:
```{r echo=TRUE, eval = FALSE, include = TRUE}
####################################################################
#Part 2 - Model 1: GBM (Gradient Boosting Machines) Classifier
#- Details and Tuning
####################################################################
library(caret)
library(gbm)
set.seed(0)
#fitting the GB classifier on train data
q2gb.fit = train(xTrain2,as.factor(yTrain2),'gbm',trControl=trainControl(method='cv',number=10))
q1gb.fit
```
```{r echo=FALSE, eval = TRUE, include = FALSE}
########################################################################
#Part 2 - Model 1: GBM (Gradient Boosting Machines) Classifier
#- Details and Tuning
#######################################################################

library(caret)
library(gbm)
set.seed(0)
#fitting the NB classifier on train data
q2gb.fit = train(xTrain2,as.factor(yTrain2),'gbm',trControl=trainControl(method='cv',number=10))
q1gb.fit
```
* **Model Validation*
```{r echo=FALSE, eval = TRUE, include = TRUE }
#######################################################################################
#Part 2 - Model 1: GBM (Gradient Boosting Machines) Classifier - Validation
#######################################################################################

#Model Evaluation
#Predict on test data & train data
q2gb.yPredTest <- predict(q2gb.fit,newdata = xTest2 )
q2gb.yPredTrain <- predict(q2gb.fit,newdata = xTrain2 )

#Test - Confusion matrix to see accuracy value and other parameter values
q2gb.tscm = confusionMatrix(q2gb.yPredTest,as.factor( yTest2) )
cat("Part 2 GBM - TEST DATA PREDICTION STATISTICS")
q2gb.tscm
#Train - Confusion matrix to see accuracy value and other parameter values
q2gb.trcm  = confusionMatrix(q2gb.yPredTrain,as.factor( yTrain2) )
#q2gb.trcm
#Accuracy on test & train
q2gb.test.acc  = 1- (sum(yTest2 != q2gb.yPredTest) / length(q2gb.yPredTest))
#q2gb.test.acc
q2gb.train.acc  = 1- (sum(yTrain2 != q2gb.yPredTrain) / length(q2gb.yPredTrain))
#q2gb.train.acc

```


### 6.2 Model 2
* **Model Details and Tuning**
Second classifier  used for Part 2 is K-Nearest Neighbor(KNN) classifier  from 'class' package is used and the output is a class membership - benign (0) or malignant(1).
Square root of data length 210 = 14.4 (hence k = 14 is chosen).

Parameters and settings chosen are same as that of KNN in Part 1, for comparison purpose.

Details of the model and prediction statistics are given below:
```{r echo=TRUE, eval = TRUE, include = TRUE }
################################################################
#Part 2 - Model 2: K Nearest Neighbor (KNN) Details and Tuning
################################################################
library(class)
set.seed(10)
#fit KNN model on train data and predict ; number of classes = 2
#Model fitting
q2knn.yPredTrain <- knn(train = xTrain2, test = xTrain2, cl=yTrain2 , k=16)
q2knn.yPredTest <- knn(train = xTrain2, test = xTest2, cl=yTrain2 , k=16)

```

* **Model Validation**
```{r echo=FALSE, include = TRUE, eval = TRUE}

################################################################
#Part 2 - Model 2: K Nearest Neighbor (KNN) Validation
################################################################
library(caret)

#Accuracy
q2knn.train.acc  = 1- (sum(yTrain2 != q2knn.yPredTrain) / length(q2knn.yPredTrain))
q2knn.test.acc  = 1- (sum(yTest2 != q2knn.yPredTest) / length(q2knn.yPredTest))

#Test - Confusion matrix to see accuracy value and other parameter values
q2knn.tscm = confusionMatrix(q2knn.yPredTest,as.factor( yTest2) )
cat("Part 2 KNN - TEST DATA PREDICTION STATISTICS")
q2knn.tscm
#Train - Confusion matrix to see accuracy value and other parameter values
q2knn.trcm = confusionMatrix(q2knn.yPredTrain,as.factor( yTrain2) )
#q2knn.trcm
```

### 6.3 Model Results for Part 1 and Part 2 - Interpretation
Below plots show the prediction accuracies of 3 classification models from Part 1 where only pixel data are used as features and for classification and 2 classification models from Part 2 where engineered features based on inputs from medical literature for skin cancer are used for classification.

KNN and GBM algorithms are used for classifying benign and malignant skin lesions, under both  Part 1 and Part 2 with the same parameters. Based on the accuracies(plotted below) , sensitivity and specificity (listed in the above sections) it is clear that feature engineering has improved the accuracy of these models).

```{r echo=FALSE, include = TRUE, eval = TRUE, out.width = '100%'}
##########################################################
#Classification Model Results Comparison (for Part 2)
##########################################################
library(ggplot2)

#q1.model.compare.trn <- data.frame(Model = c('KNN', 'Gradient Boosting','Naive Bayes'),Accuracy = c( q1knn.trcm$overall[1],q1gb.trcm$overall[1],q1nb.trcm$overall[1]))

q1.model.compare.tst <- data.frame(Model = c('KNN', 'Gradient Boosting','Naive Bayes'),
                            Accuracy = c( q1knn.tscm$overall[1],
                                         q1gb.tscm$overall[1],
                                         q1nb.tscm$overall[1]))

#q2.model.compare.trn <- data.frame(Model = c('KNN', 'Gradient Boosting'), Accuracy = c( q2knn.trcm$overall[1],q2gb.trcm$overall[1]))

q2.model.compare.tst <- data.frame(Model = c('KNN', 'Gradient Boosting'),
                            Accuracy = c(q2knn.tscm$overall[1],
                                         q2gb.tscm$overall[1]))

par(mfrow = c(1,2))


ggplot(aes(x=Model, y=Accuracy), data=q1.model.compare.tst) +
    geom_bar(stat='identity', fill = 'lightblue',width = .35) +
    ggtitle('Comparative Classification Accuracy of Part1 Models on Test Data') +
    xlab('Models') +
    ylab('Overall Accuracy')+
    geom_text(aes(label=Accuracy*100))

ggplot(aes(x=Model, y=Accuracy), data=q2.model.compare.tst) +
    geom_bar(stat='identity', fill = 'lightgreen',width = .35) +
    ggtitle('Comparative Classification Accuracy of Part2 Models on Test Data') +
    xlab('Models') +
    ylab('Overall Accuracy')+
    geom_text(aes(label=Accuracy*100))


```
The increase in accuracy can obviously attributed to the fact that additional features like Area, Perimeter, Border Asymmetry index, Geometrical asymmetry etc that relate to the ‘ABCDE’ features for identifying skin cancer has been used in training Part 2 models where as Part 1 models were just trained on pixel data alone and nothing else. Incorporating domain specific features have indeed increased the accuracy.
